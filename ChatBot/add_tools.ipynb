{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "web_search = TavilySearch(max_results=2)\n",
    "tools = [web_search]\n",
    "# web_search.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd1a0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b650034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c41ace066f0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Modification: tell the LLM which tools it can call\n",
    "# 对于在第一个教程中创建的 StateGraph ，在 LLM 上添加 bind_tools\n",
    "# 这能让 LLM 知道如果它想要使用tools中的工具，应该使用正确的 JSON 格式\n",
    "# 决定是否调用工具由llm决定\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # 把整个state的message都传过去，相当于为llm增加记忆\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c41ad05fa70>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在，创建一个函数来运行被调用的工具。通过将工具添加到一个名为 BasicToolNode \n",
    "# 的新节点来实现，该节点检查状态中的最新消息，如果消息包含 tool_calls ，则调用\n",
    "# 工具。它依赖于 LLM 的 tool_calling 支持\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict): # 这里写 dict 只是类型提示\n",
    "        # 实际上，无论传入什么类型，__call__ 都会执行\n",
    "        # 但是下面这行代码要求 inputs 必须有 .get() 方法（即字典）\n",
    "        # 使用get方法，遇到不存在的key会返回None或者默认值，使用[]遇到不存在key会报key error\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "# 这里创建实例调用init\n",
    "tool_node = BasicToolNode(tools=[web_search])\n",
    "# result = tool_node(some_input)  # ← 这里才是调用 __call__\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9391ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义 conditional_edges\n",
    "边将控制流从一个节点路由到下一个节点。条件边从一个节点开始，通常包含\"if\"语句，根据当前图状态路由到不同的节点。\n",
    "这些函数接收当前图 state 并返回一个字符串或字符串列表，指示下一个要调用的节点。\n",
    "\n",
    "接下来，定义一个名为 route_tools 的路由函数，该函数用于检查聊天机器人的输出中是否包含 tool_calls。\n",
    "通过调用 add_conditional_edges 将此函数提供给图，这会告诉图每当 chatbot 节点完成时，需要检查此函数以确定下一步的去向。\n",
    "\n",
    "条件存在工具调用时将路由到 tools ，不存在时将路由到 END 。因为条件可以返回 END ，所以这次不需要显式设置 finish_point\n",
    "'''\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"edge_tools\"\n",
    "    return \"edge_end\"\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"edge_tools\" if the chatbot asks to use a tool, and \"edge_end\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",  # 源节点名称\n",
    "    route_tools,    # 决定路径的函数\n",
    "    {\"edge_tools\": \"tools\", \"edge_end\": END}, # 路径映射字典，根据route_tools返回的结果确定下一个Node\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "# 将 tools 和 chatbot 这两个bot相连\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "# 将 chatbot 与 开始节点相连\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8e795ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd901ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \n",
      "Assistant: {\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent\\u2019s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.942002, \"raw_content\": null}, {\"url\": \"https://langchain-ai.github.io/langgraphjs/\", \"title\": \"LangGraph.js\", \"content\": \"LangGraph \\u2014 used by Replit, Uber, LinkedIn, GitLab and more \\u2014 is a low-level orchestration framework for building controllable agents\", \"score\": 0.8925721, \"raw_content\": null}], \"response_time\": 1.14, \"request_id\": \"94a04119-5329-401f-827a-5bfdc32fb953\"}\n",
      "Assistant: LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow, allowing full transparency of the agent’s state. LangGraph is used by companies like Replit, Uber, LinkedIn, and GitLab. In LangGraph, nodes represent individual components or agents within an AI workflow and uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now you can ask the chatbot questions outside its training data\n",
    "'''\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 构建state schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 初始化工具\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "\n",
    "# 初始化模型 绑定工具\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 定义Node\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "# 增加Node\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 定义工具Node\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "# 增加工具Node\n",
    "graph_builder.add_node(\"my_tools\", tool_node)\n",
    "\n",
    "# 增加条件边\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,    # langgraph自带的工具条件函数返回字符串是写死的，因此下面的映射字典，只能写tools和__end__\n",
    "    {\"tools\": \"my_tools\", \"__end__\": END} # 如果不需要工具调用 直接结束\n",
    ")\n",
    "# 增加普通边\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"my_tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
